# Homework8
## 1. 简述为什么会有Spark
&emsp;&emsp;就大数据计算模式发展而言。在2013年，大数据计算模式发生了新的变化，大量非批处理问题的出现，使得人们开始意识到MapReduce对非批处理问题的局限性。人们开始寻找一种新的计算模式与系统，这种计算模式与系统能够同时解决大数据批处理或非批处理问题。于是，以内存计算为核心、集多种计算模式之大成的Spark生态系统应运而生，该生态系统不仅具有MapReduce计算模式具有的大数据查询计算和批处理计算功能，还能够进行流式计算、迭代计算、图计算、内存计算等。  
&emsp;&emsp;就Spark产生及发展过程而言。Spark是加州大学伯克利分校的AMP实验室在2009年开发的通用并行计算框架，并在2010年开放源代码。之后，Spark由于其在大数据处理方面的优越性而称为Apache的顶级项目。后来，围绕Spark推出了Spark SQL、Spark Streaming、MLlib和GraphX等组件，使其逐渐成为大数据处理的一站式平台。  
&emsp;&emsp;就Spark数据处理的逻辑核心而言。Spark对数据的处理基于内存的弹性分布式数据集（RDD），该设计使得计算的中间结果保存在内存而非磁盘上，从而因避免了磁盘读取而大大提高了处理性能。同时，RDD还能很好地支持迭代计算的处理问题，比如一组RDD形成可执行的有向无环图DAG，构成灵活的计算流图。  
&emsp;&emsp;就Spark的特性而言。Spark计算模式具有众多突出特性，其默认使用函数式语言Scala，大大减少了编码量，同时，其还支持多种编程语言如Python、Java、R等。Spark同时具备SQL、流处理、复杂分析和机器学习等功能，用户可以在使用Spark的过程中“无缝结合”这些工程。此外，Spark具有通用性，同一个Spark程序可以在多种模式（如单机模式、伪分布模式、集群模式等）或多种生态系统（如Hadoop生态系统）下正常运行。
## 2. 对比Hadoop和Spark
## 3. 简述Spark的技术特点

# Homework8
## 1. 简述为什么会有Spark
&emsp;&emsp;就大数据计算模式发展而言。在2013年，大数据计算模式发生了新的变化，大量非批处理问题的出现，使得人们开始意识到MapReduce对非批处理问题的局限性。人们开始寻找一种新的计算模式与系统，这种计算模式与系统能够同时解决大数据批处理或非批处理问题。于是，以内存计算为核心、集多种计算模式之大成的Spark生态系统应运而生，该生态系统不仅具有MapReduce计算模式具有的大数据查询计算和批处理计算功能，还能够进行流式计算、迭代计算、图计算、内存计算等。  
&emsp;&emsp;就Spark产生及发展过程而言。Spark是加州大学伯克利分校的AMP实验室在2009年开发的通用并行计算框架，并在2010年开放源代码。之后，Spark由于其在大数据处理方面的优越性而称为Apache的顶级项目。后来，围绕Spark推出了Spark SQL、Spark Streaming、MLlib和GraphX等组件，使其逐渐成为大数据处理的一站式平台。  
&emsp;&emsp;就Spark数据处理的逻辑核心而言。Spark对数据的处理基于内存的弹性分布式数据集（RDD），该设计使得计算的中间结果保存在内存而非磁盘上，从而因避免了磁盘读取而大大提高了处理性能。同时，RDD还能很好地支持迭代计算的处理问题，比如一组RDD形成可执行的有向无环图DAG，构成灵活的计算流图。  
&emsp;&emsp;就Spark的特性而言。Spark计算模式具有众多突出特性，其默认使用函数式语言Scala，大大减少了编码量，同时，其还支持多种编程语言如Python、Java、R等。Spark同时具备SQL、流处理、复杂分析和机器学习等功能，用户可以在使用Spark的过程中“无缝结合”这些工程。此外，Spark具有通用性，同一个Spark程序可以在多种模式（如单机模式、伪分布模式、集群模式等）或多种生态系统（如Hadoop生态系统）下正常运行。
## 2. 对比Hadoop和Spark
### Hadoop与Spark的主要区别在于，Hadoop使用持久（本地化）存储，Spark使用弹性分布式数据集。  
&emsp;&emsp;就性能和中间数据存放位置而言，Hadoop需要把MapReduce得到的计算结果存放在硬盘上，哪怕该计算结果仅为任务中的中间结果；而Spark将任务的中间数据存放在内存中，避免了内存再次读取硬盘上的数据，大大提高了任务处理速度。如Spark支持DAG图的分布式并行计算，具体实现是通过一组RDD形成可执行的有向无环图，构成灵活的计算流图，避免了中间数据落地；而Hadoop则需要通过多个MapReduce过程进行读写硬盘处理。  
&emsp;&emsp;就容错性而言，Spark引入了RDD抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，即Spark系统通过血统关系（lineage）来记录一个RDD是如何通过其他一个或者多个父类RDD转变过来的，当这个RDD的数据丢失时，Spark可以通过它父类的的RDD重新计算，从而实现数据的可靠性。相比而言，Hadoop只通过Namenode和datanode保证任务不出错，但无法保证磁盘上存储的数据的鲁棒性。  
&emsp;&emsp;就功能范围而言，Spark更加通用。在Hadoop中，一个Job只有Map和Reduce两个阶段，复杂的计算需要大量的Job来完成，Job之间的依赖关系是由开发者自己管理的；Spark提供的数据集操作更多，另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种，用户可以命名、物化、控制中间结果的存储、分区等，复杂的计算也可以通过一组RDD形成可执行的有向无环图来进行流计算。
## 3. 简述Spark的技术特点
